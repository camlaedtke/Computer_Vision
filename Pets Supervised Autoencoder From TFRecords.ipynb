{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Using Preprocessed TFRecords Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import PIL\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers, models\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "K.clear_session()\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Automatic mixed precision gives around a 2X train speed-up and allows for 2X larger batch size. \n",
    "# Requires final layer be manually cast to float32\n",
    "def enable_amp():\n",
    "    # tf.config.optimizer.set_jit(True)\n",
    "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "    mixed_precision.set_policy(policy)\n",
    "    \n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(physical_devices)\n",
    "enable_amp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature((), tf.string),\n",
    "        'segmentation': tf.io.FixedLenFeature((), tf.string),\n",
    "        'height': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'width': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'image_depth': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'mask_depth': tf.io.FixedLenFeature((), tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "    image = tf.io.parse_tensor(example['image'], out_type = float)\n",
    "    image_shape = [example['height'], example['width'], 3]\n",
    "    image = tf.reshape(image, image_shape)\n",
    "    \n",
    "    mask = tf.io.parse_tensor(example['segmentation'], out_type = tf.uint8)\n",
    "    mask_shape = [example['height'], example['width'], 1]\n",
    "    mask = tf.reshape(mask, mask_shape)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def get_dataset_from_tfrecord(tfrecord_dir):\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_dir)\n",
    "    parsed_dataset = tfrecord_dataset.map(read_tfrecord)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_dir = 'oxford_pets\\\\train.tfrecords'\n",
    "test_tfrecord_dir = 'oxford_pets\\\\test.tfrecords'\n",
    "\n",
    "img_height = 384\n",
    "img_width = 384\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mask_to_categorical(image, mask):\n",
    "    mask = tf.squeeze(mask)\n",
    "    mask = tf.one_hot(tf.cast(mask, tf.int32), n_classes)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image_train(input_image, input_mask):\n",
    "    input_image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    input_mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "def load_image_test(input_image, input_mask):\n",
    "    input_image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    input_mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "    \n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 5912\n",
    "TEST_LENGTH = 1478\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = TRAIN_LENGTH \n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecords_dataset = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "test_tfrecords_dataset = get_dataset_from_tfrecord(test_tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: resize the images and masks, flip them, and normalize them\n",
    "train = train_tfrecords_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = test_tfrecords_dataset.map(load_image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cache() transformation reduces resource usage\n",
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in test.take(13):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    \n",
    "sample_mask = tf.argmax(sample_mask, axis=-1)\n",
    "sample_mask = sample_mask[..., tf.newaxis]\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_height=img_height,  input_width=img_width, n_classes = 3):\n",
    "    \n",
    "    img_input = tf.keras.layers.Input(shape=(input_height, input_width, 3))\n",
    "\n",
    "    # -------------------------- Encoder --------------------------\n",
    "    \n",
    "    c1 = Conv2D(64, 3, padding='same', activation=\"selu\")(img_input)\n",
    "    c1 = Conv2D(64, 3, padding='same', activation=\"selu\")(c1)\n",
    "    p1 = MaxPooling2D((2,2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, 3, padding='same', activation=\"selu\")(p1)\n",
    "    c2 = Conv2D(128, 3, padding='same', activation=\"selu\")(c2)\n",
    "    p2 = MaxPooling2D((2,2))(c2)\n",
    "    p2 = Dropout(0.1)(p2)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, padding='same', activation=\"selu\")(p2)\n",
    "    c3 = Conv2D(256, 3, padding='same', activation=\"selu\")(c3)\n",
    "    p3 = MaxPooling2D((2,2))(c3)\n",
    "    p3 = Dropout(0.2)(p3)\n",
    "    \n",
    "    c4 = Conv2D(512, 3, padding='same', activation=\"selu\")(p3)\n",
    "    c4 = Conv2D(512, 3, padding='same', activation=\"selu\")(c4)\n",
    "    p4 = MaxPooling2D((2,2))(c4)\n",
    "    p4 = Dropout(0.3)(p4)\n",
    "    \n",
    "    # ------------------------ Bottleneck -------------------------\n",
    "    \n",
    "    c5 = Conv2D(1024, 3, padding='same', activation=\"selu\")(p4)\n",
    "    c5 = Conv2D(1024, 3, padding='same', activation=\"selu\")(c5)\n",
    "    c5 = Dropout(0.5)(c5)\n",
    "    \n",
    "    # -------------------------- Decoder --------------------------\n",
    "    # Can also use transpose convolutions instead of upsampling\n",
    "    \n",
    "    u6 = concatenate([UpSampling2D(2)(c5), c4])\n",
    "    c6 = Conv2D(512, 3, padding='same')(u6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Activation('selu')(c6)\n",
    "    c6 = Conv2D(256, 3, padding='same')(c6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Activation('selu')(c6)\n",
    "    c6 = Dropout(0.3)(c6)\n",
    "    \n",
    "    u7 = concatenate([UpSampling2D(2)(c6), c3])\n",
    "    c7 = Conv2D(256, 3, padding='same')(u7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Activation('selu')(c7)\n",
    "    c7 = Conv2D(128, 3, padding='same')(c7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Activation('selu')(c7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "\n",
    "    u8 = concatenate([UpSampling2D(2)(c7), c2])\n",
    "    c8 = Conv2D(128, 3, padding='same')(u8)\n",
    "    c8 = BatchNormalization()(c8)\n",
    "    c8 = Activation('selu')(c8)\n",
    "    c8 = Conv2D(64, 3, padding='same')(c8)\n",
    "    c8 = BatchNormalization()(c8)\n",
    "    c8 = Activation('selu')(c8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "\n",
    "    u9 = concatenate([UpSampling2D(2)(c8), c1]) \n",
    "    c9 = Conv2D(64, 3, padding='same')(u9)\n",
    "    c9 = BatchNormalization()(c9)\n",
    "    c9 = Activation('selu')(c9)\n",
    "    c9 = Conv2D(64, 3, padding='same')(u9)\n",
    "    c9 = BatchNormalization()(c9)\n",
    "    c9 = Activation('selu')(c9)\n",
    "    c9 = Conv2D(n_classes, 3, padding='same')(c9)\n",
    "    \n",
    "    # Softmax for multi-class prediction\n",
    "    output = Activation(\"softmax\", dtype='float32')(c9)\n",
    "    \n",
    "    return tf.keras.Model(inputs=img_input, outputs=output)   \n",
    "\n",
    "\n",
    "\n",
    "def xception_model(input_height=img_height,  input_width=img_width, n_classes = 4):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(input_height, input_width, 3))\n",
    "\n",
    "    ######## [First half of the network: downsampling inputs] ########\n",
    "\n",
    "    # Entry block\n",
    "    x = Conv2D(64, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"selu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [128, 256, 512]:\n",
    "        x = Activation(\"selu\")(x)\n",
    "        x = SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation(\"selu\")(x)\n",
    "        x = SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ######## [Second half of the network: upsampling inputs] ########\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for filters in [512, 256, 128, 64]:\n",
    "        x = Activation(\"selu\")(x)\n",
    "        x = Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation(\"selu\")(x)\n",
    "        x = Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = UpSampling2D(2)(previous_block_activation)\n",
    "        residual = Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    x = Conv2D(n_classes, 3, padding=\"same\")(x)\n",
    "    \n",
    "    outputs = Activation(\"softmax\", dtype='float32')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    \"\"\"Pixel-wise dice coefficient calculated for each class and averaged over batch. Final dice score is the average \n",
    "    of the dice scores for each class.\"\"\"\n",
    "    dice = 0.0\n",
    "    smooth = 1.0\n",
    "    for i in range(1, n_classes):\n",
    "        intersection = y_true[:,:,i] * y_pred[:,:,i]\n",
    "        all_ = y_true[:,:,i] + y_pred[:,:,i]\n",
    "        intersection = K.sum(intersection, 1)\n",
    "        all_ = K.sum(all_, 1)\n",
    "        temp = (2. * intersection + smooth) / (all_ + smooth)\n",
    "        temp = K.mean(temp)\n",
    "        dice = dice + temp\n",
    "    return dice / (n_classes-1)\n",
    "\n",
    "\n",
    "# Custom loss function combines categorical cross entropy and pixel-wise dice coefficient. Dice loss is especially good for \n",
    "# datasets with class-imbalance\n",
    "def cce_dice_loss(y_true, y_pred):\n",
    "    return (tf.keras.losses.categorical_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = unet_model(input_height=img_height, input_width=img_width, n_classes=4)\n",
    "# model = xception_model(input_height=img_height, input_width=img_width, n_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.squeeze(pred_mask, axis=0)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def show_predictions():\n",
    "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
    "    display([sample_image, sample_mask, create_mask(pred_mask)])\n",
    "\n",
    "        \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "        \n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"saved_models\\\\unet_pets.h5\"\n",
    "\n",
    "model.compile(optimizer = Adam(lr=1e-4),\n",
    "              loss = cce_dice_loss, \n",
    "              metrics = ['accuracy', dice_coef])\n",
    "\n",
    "callbacks = [\n",
    "    DisplayCallback(),\n",
    "    EarlyStopping(monitor='val_loss', mode='min', patience=9, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', mode='min', patience=3, factor=0.1, min_lr=1e-10, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', verbose=1, mode='min', save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "# model.load_weights(\"big_unet_model.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "VALIDATION_STEPS = TEST_LENGTH//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = model.fit(train_dataset,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_data = test_dataset,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose = 1)\n",
    "end_time = time.time()\n",
    "\n",
    "t_minutes = (end_time - start_time) // 60\n",
    "t_per_epoch = (end_time - start_time)  // len(results.history[\"loss\"])\n",
    "print(\"#### time: {} min at {} s per epoch\".format(t_minutes, t_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"saved_models\\\\unet_pets.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,3,1)  \n",
    "plt.plot(results.history['loss'], 'r', label='Training loss')\n",
    "plt.plot(results.history['val_loss'], 'b', label='Validation loss')\n",
    "plt.title('Log Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('log_loss', fontsize=16)\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(results.history['accuracy'], 'r', label='Training accuracy')\n",
    "plt.plot(results.history['val_accuracy'], 'b', label='Validation accuracy')\n",
    "plt.title('Accuracy', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(results.history['dice_coef'], 'r', label='Dice coefficient')\n",
    "plt.plot(results.history['val_dice_coef'], 'b', label='Validation dice coefficient')\n",
    "plt.title('Dice Coefficient', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Dice', fontsize=16)\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "X_test = np.zeros((n_samples, img_height, img_width, 3))\n",
    "y_test = np.zeros((n_samples, img_height, img_width, 4))\n",
    "\n",
    "for idx, (image, mask) in enumerate(test):\n",
    "    X_test[idx] = image.numpy()\n",
    "    y_test[idx] = mask.numpy()\n",
    "    if idx == (n_samples-1):\n",
    "        break\n",
    "\n",
    "print(\"X_test.shape: {} , y_test.shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 38\n",
    "sample_image = X_test[img_num]\n",
    "sample_mask = np.expand_dims(np.argmax(y_test[img_num], axis=-1), axis=-1)\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.squeeze(pred_mask, axis=0)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def show_predictions():\n",
    "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
    "    display([sample_image, sample_mask, create_mask(pred_mask)])\n",
    "    \n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test[0:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick way to evaluate model using dice coefficient evaluated over larger set of datapoints\n",
    "def dice_coef_eval(y_true, y_pred):\n",
    "    dice = 0.0\n",
    "    smooth = 1.0\n",
    "    class_dice = []\n",
    "    for i in range(1, n_classes):\n",
    "        intersection = y_true[:,:,i] * y_pred[:,:,i]\n",
    "        all_ = y_true[:,:,i] + y_pred[:,:,i]\n",
    "        intersection = K.sum(intersection, 1)\n",
    "        all_ = K.sum(all_, 1)\n",
    "        temp = (2. * intersection + smooth) / (all_ + smooth)\n",
    "        temp = K.mean(temp)\n",
    "        class_dice.append(round(temp.numpy(), 4))\n",
    "        dice = dice + temp\n",
    "    total_dice = dice / (n_classes-1)\n",
    "    return class_dice, round(total_dice.numpy(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dice, total_dice = dice_coef_eval(y_test[0:n_samples], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"class 1: {} \\nclass 2: {} \\nclass 3: {} \\ntotal: {}\".format(class_dice[0], class_dice[1], class_dice[2], total_dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.16\n",
    "# 89.14\n",
    "# 87.18\n",
    "# 90.13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
